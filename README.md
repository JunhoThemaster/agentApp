# 🤖 Multimodal Robot Data Search Agent

## 📌 프로젝트 개요
이 프로젝트는 **DROID Dataset 1.0.1**을 기반으로 시작되었습니다.  
처음에는 로봇 시퀀스 데이터를 분석하고, 머신러닝/딥러닝 모델을 적용하여 동작 분류를 시도했으나  
**라벨 경계가 애매하고 데이터 특성이 불분명**하여 모델링 성능에 한계를 경험했습니다.  

이에 따라 단순 분류 모델링을 넘어,  
👉 **대량의 로봇 센서 데이터와 영상을 멀티모달하게 검색할 수 있는 Agent Tool**을 개발하게 되었습니다.  

---

## 🎯 문제 정의
- 로봇 연구자가 직면하는 가장 큰 어려움 중 하나는  
  **“방대한 데이터에서 특정 동작이나 시퀀스를 빠르게 찾아내는 것”** 입니다.  

- DROID와 같은 대규모 로봇 데이터셋은 수천~수만 개의 시퀀스와 영상으로 이루어져 있어,  
  사람이 직접 탐색하기에는 **매우 비효율적이고 반복적**입니다.  

- 또한 로봇 동작의 품질을 개선하려면, **센서 로그에서 도출된 통계 정보**를 직관적으로 파악할 수 있어야 합니다.  
  이를 통해 엔지니어는 문제의 원인을 **하드웨어(HW) 또는 소프트웨어(SW)** 관점에서 신속하게 찾을 수 있습니다.  

- 더 나아가, 제조업 강국인 대한민국은 **인구 절벽 문제**로 인해 노동력 부족 현상이 심화되고 있으며,  
  이에 따라 로봇의 일자리 대체는 **불가피한 흐름**이 되고 있습니다.  

- 따라서, **AI 기반 자동화의 품질 개선과 문제 포착을 손쉽게 지원하는 에이전트 툴**이 필요하다고 판단했습니다.  

---

## 🔍 데이터 분석 과정

- **초기 접근**
  - 원본 DROID Dataset 1.0.1은 로봇 시퀀스, 영상, 센서 로그(관절/카르테시안 좌표 (데카르트 좌표계를 따름)로 구성되어 있습니다.
  - 연구소 별로 샘플을 뽑아본 결과 각기 다른 컬럼의 수, 품질의 차이 (최소 프레임,센서데이터 nan값 유무,메타데이터 설명 부실 등)의 문제를 해결하기위해
    연구소의 선정도 필요했고, **고정된 스키마**가 요구 되었습니다.
  - 동작 라벨(ground truth)이 명확하지 않아, **행동 단위 라벨을 생성**할 필요가 있다고 판단했습니다.

- **요인분석 (Factor Analysis) 수행**
  - 센서 데이터의 고차원 변수를 줄이고 행동을 설명하는 잠재 요인을 찾고 고정된 스키마를 수립하기위해 요인분석을 실시했습니다.  
  - Bartlett 구형성 검정 및 KMO 측도를 사용하여 요인분석 적합성을 검증  
    ![요인분석 적합성 테스트 결과](https://github.com/user-attachments/assets/74c0decc-675d-4c53-a1cc-be650e4ab947)

  - Bartlett 검정: 유의확률 p < 0.001 → 변수들 간 상관관계가 유의미함  
  - KMO overall = 0.632 → **요인분석이 가능하지만 다소 경계선 수준**
  - - → 변수 간 상관성이 매우 강하여 공분산 행렬이 수치적으로 불안정하게 계산되었습니다. 
  - 하지만 p-value가 유의하므로, 요인분석 자체는 적용 가능하다고 판단  
  - 이 결과를 기반으로, 센서 데이터에서 주요 행동 요인을 도출하고
    공선성이 강한 변수들을 제거하는 절차를 진행했습니다.
 

- **라벨 생성 (`desc_major`)**
  - 요인분석 및 센서 패턴 분석을 종합하여 직접 라벨링을 수행하기 위한 **라벨링 툴**을 제작했습니다.
     - ![수동 라벨러](https://github.com/user-attachments/assets/b95b4f61-ce1c-4760-8c27-8bd6abe7162f)


  - `"대기"`, `"파지"`, `"운반"`, `"내려놓기"`, `"정렬"`, `"이탈"`, `"완료"` 등 주요 행동 단위로 구분  
  - → 최종적으로 데이터셋에 **`desc_major` 컬럼**을 생성  

- **시각화 검증**
  -  라벨링한 결과를 토대로 Cartesian Position/Velocity 그래프에 `desc_major` 경계를 표시하여 비교  
  - 실제 센서 패턴은 연속적이고 선형적인 경우도 있는데 , 라벨은 특정 시점에서 뚝 끊겨 바뀌는 경우 다수 발견, 동작별 일관된 패턴 거의 포착불가, 즉 **경계가 애매하다** 라는 결론에 도달했습니다.
  - → 이로 인해 분류 모델은 성능이 제한적일 수 있음을 예상했습니다 아래는 좌표계의 변화에 따른 라벨의 변화 입니다.
      ![좌표계의 변화에 따른 라벨 변화](https://github.com/user-attachments/assets/c7603d61-e75e-41cb-8595-015064ce08ee)



---

---

## 🧪 시도한 접근 & 결과

---

### ⚠️ 한계/실패한 시도


🤖**머신러닝 기반 센서 시퀀스 분류 (오토라벨링) 모델 제작**
  - `XGBoost`
  - **적용 이유**:
    - 센서 로그를 구간 단위로 집계하면 시계열성이 약화되므로, 트리 기반 모델이 적합하다고 판단
    - 클래스 불균형에도 비교적 강건하고, 과적합 방지를 위한 정규화 기능이 탑재
    - 무엇보다 **Feature Importance**를 제공하여, 고차원 센서 피처 중 유효한 변수를 선별할 수 있음
  - **시도**: 센서 시퀀스를 구간 단위(feature 집계)로 변환 후 분류 학습
  - **결과**: 일정 수준의 성능은 보였으나,
    - **라벨 경계가 애매**하여 안정적 정확도 확보 실패
    - XGBoost는 비교적 나은 성능을 보였지만 여전히 분류 한계 존재
  - **인사이트**: 센서 데이터만으로는 “동작 전환 경계” 포착의 한계  
    - 학습한 도메인 내에서는 성능이 괜찮았지만, 다른 도메인으로 일반화하면 동작을 구분 X
  - 추가 성과: **Feature Importance 분석**을 통해 수많은 센서 컬럼 중 유효한 특성을 선별할 수 있었음 → 이후 검색/탐색 설계에 참고됨

🤖**딥러닝 기반 센서 시퀀스 분류**
  - `SensorLSTMAttnClassifier` (LSTM + Attention Pooling)
  - **적용 이유**:
    - 센서 데이터는 본질적으로 시계열(Time-series) 특성을 가지므로, 순차적 패턴 학습이 필요
    - LSTM은 시계열 데이터의 시간적 의존성을 학습하는 데 강점이 있음
    - Attention Pooling을 추가하여, 시퀀스 내 중요한 타임스텝에 가중치를 주어 행동 구분 성능을 높이고자 함
  - **시도**: 시퀀스 전체를 입력받아 Attention 기반 요약 후 `desc_major` 라벨 분류
  - **강화**: 센서 수치형 컬럼에 jitter값을 추가해 약간의 노이즈를 0.01 부여했음, 이로인해 경계가 좀 더 뚜렷하게 보이기를 기대함
  - **결과**: 구조적으로는 정상 작동했으나
    - **라벨 경계 불명확성**으로 학습 안정성이 낮음
    - **클래스 불균형**으로 특정 라벨에 치우친 결과 발생
    - 장기 시퀀스 일반화 부족 (다른 도메인 데이터로 확장 시 성능 급격히 저하)
  - **인사이트**:
    - 단순 분류 모델로는 센서 데이터의 한계를 극복하기 어려움
    - 현재 센서 데이터는 분류보다는 **검색/탐색 기반 활용**이 더 적합하다는 결론
 #### 📉 센서 기반 분류 결과
   xgboost 와 LSTM Attention 모델을 적용했으나, 라벨 경계 문제와 클래스 불균형으로 인해 성능이 제한적이었습니다.
   
   ![로봇행동 분류모델 성능 표](https://github.com/user-attachments/assets/15f0ca71-c892-4ad2-8f56-b52dafbd985a)
   
   
   ![로봇행동 분류모델 혼동행렬](https://github.com/user-attachments/assets/be34a600-0134-4bbc-91c3-0d80320f47d1)
   
   ![로봇행동 분류모델 혼동행렬 분석](https://github.com/user-attachments/assets/acc93819-e12c-4864-9d26-bf3ac2b110d5)
   
   - 일부 클래스(F1 ≈ 0.7)는 양호했지만, `"정렬"`과 같은 라벨은 F1 ≈ 0.2 수준  
   - 전체 accuracy ≈ 0.50 → **실제 적용하기에는 부족**
   - 두 모델의 차이가 크지않았기에 사진 1장으로 설명합니다.
   - ➡️ 분류보다는 검색 접근이 더 적합함을 확인  

🤖**Gemma3를 이용한 비디오 캡셔닝 및 임베딩**
  - 모델: **Gemma3n (4B pt)**  
  - **적용 이유**:
    - 수동 라벨링 기반 접근의 성능이 만족스럽지 않았음
    - 따라서 **멀티모달 LLM인 Gemma3**를 활용하여,
      - 비디오를 프레임 단위로 나누어 캡셔닝(Text 생성)
      - 생성된 텍스트를 임베딩 모델에 전달하여 검색에 활용
    - Gemma3는 이미지 입력과 다국어 처리(한국어 포함)를 지원한다는 점에서 적합하다고 판단  
      ![Gemma3 Developer Guide](https://developers.googleblog.com/ko/introducing-gemma-3n-developer-guide/)
  - **시도**:
    - Hugging Face를 통해 로컬 환경에 모델을 탑재
    - 비디오 → 프레임 이미지 분할 → Gemma3를 이용한 캡션 생성 → 텍스트 임베딩 추출 파이프라인 구현 시도
  - **결과**:
    - 내부 자원(GPU 메모리) 부족으로 실행 4~5분 후 시스템 프리징 현상 발생
    - → 로컬 환경에서는 실험 불가
  - **인사이트**:
    - **가용 자원에 맞는 모델 선택**의 중요성을 확인
    - 추후 확장성을 고려한다면, 경량화 모델 또는 클라우드/분산 환경에서의 학습·추론이 필요

    - **Instruction Tuning (인스트럭션 튜닝)**
      - 현재 Gemma3는 기본 이미지/텍스트 멀티모달 처리에 강점이 있음
      - 추후 LoRA 기반 경량 튜닝을 통해, 
        - 비디오 프레임 → 캡션 생성 품질을 향상시키고
        - 한국어 기반 질의응답/검색 품질도 개선할 예정
    
    - **Quantization (양자화)**
      - 로컬 환경에서 실행 시 GPU/메모리 자원 한계로 문제가 발생했음
      - 4bit / 8bit 양자화를 적용하여 메모리 사용량을 줄이고,
        - 경량화된 모델 실행
        - 추론 안정성 확보
      - 이를 통해 프리징/리부팅 문제를 해결하고, 실험 반복성을 보장할 수 있을 것이라 예측
    
    - **멀티모달 통합 강화**
      - 센서 데이터 + 영상 + 텍스트를 동시에 활용하는 구조 확립
      - 검색뿐 아니라 **설명 가능한 로봇 동작 분석**까지 지원하는 방향으로 확장

 

---

## 🚀 새로운 방향 
이 프로젝트는 **멀티모달 검색 에이전트**를 제공합니다.  

- **입력**: 텍스트, 키워드, 혹은 예시 영상/센서 구간  
- **검색 대상**: 로봇 센서 데이터 (Cartesian Position, Velocity, Joint State 등) + 동기화된 영상  
- **출력**: 쿼리와 가장 유사한 Top5 영상 + 해당 영상의 센서 데이터 통계 정보  

---

## ✨ 기대 효과
- 🔍 **효율적인 탐색**  
  방대한 로봇 데이터셋에서 원하는 동작을 빠르게 찾을 수 있음  
- 🎥 **멀티모달 검색**  
  텍스트 + 영상 + 센서 신호를 동시에 활용하여 검색 정밀도 향상  
- 🛠 **엔지니어링 생산성 향상**  
  반복적이고 시간이 많이 드는 데이터 탐색 과정을 자동화  
- 📈 **연구/실험 가속화**  
  모델 학습이나 분석에 필요한 데이터를 더 쉽게 수집 가능  

---

---

## 💻 개발 환경

- **OS**: Ubuntu 22.04 LTS  
- **Language**: Python 3.10 / TypeScript (React)  
- **Frameworks**: FastAPI, Node.js 18+  
- **ML/DL**: PyTorch, scikit-learn, Hugging Face Transformers,
- **Database / Storage**: Elasticsearch 8.x  
- **Data Pipeline**: Logstash (ETL 기반 인제스천)  
- **Version Control**: GitHub  

> ⚙️ 모든 실험과 개발은 Ubuntu 22.04 LTS 환경에서 수행되었습니다.

---

## 📌 결론
이 프로젝트는 **"로봇 동작 데이터의 애매한 경계 문제"**에서 출발했지만  
결국에는 **대량의 로봇 데이터와 영상에서 원하는 행동을 빠르게 찾는 검색 Agent**로 발전했습니다.  

👉 이를 통해 확인한 점:  
- 단순 분류 접근은 라벨/데이터 한계로 인해 **상용 수준의 성능을 내기 어려움**  
- 그러나 검색 기반 접근은 **엔지니어가 실제로 필요로 하는 문제 해결에 직결**  
- 따라서, 이 프로젝트는 **로봇 데이터 활용의 실질적 가치를 극대화**하는 방향으로 자리 잡았습니다.
