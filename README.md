<img width="801" height="379" alt="xgboost + 로봇 행동 분류 성능" src="https://github.com/user-attachments/assets/e12caff4-1924-4833-a637-e8251f87c571" /># 🤖 Multimodal Robot Data Search Agent

## 📌 프로젝트 개요
이 프로젝트는 **DROID Dataset 1.0.1**을 기반으로 시작되었습니다.  
처음에는 로봇 시퀀스 데이터를 분석하고, 머신러닝/딥러닝 모델을 적용하여 동작 분류를 시도했으나  
**라벨 경계가 애매하고 데이터 특성이 불분명**하여 모델링 성능에 한계를 경험했습니다.  

이에 따라 단순 분류 모델링을 넘어,  
👉 **대량의 로봇 센서 데이터와 영상을 멀티모달하게 검색할 수 있는 Agent Tool**을 개발하게 되었습니다.  

---

## 🎯 문제 정의
- 로봇 연구자가 직면하는 가장 큰 어려움 중 하나는  
  **“방대한 데이터에서 특정 동작이나 시퀀스를 빠르게 찾아내는 것”** 입니다.  

- DROID와 같은 대규모 로봇 데이터셋은 수천~수만 개의 시퀀스와 영상으로 이루어져 있어,  
  사람이 직접 탐색하기에는 **매우 비효율적이고 반복적**입니다.  

- 또한 로봇 동작의 품질을 개선하려면, **센서 로그에서 도출된 통계 정보**를 직관적으로 파악할 수 있어야 합니다.  
  이를 통해 엔지니어는 문제의 원인을 **하드웨어(HW) 또는 소프트웨어(SW)** 관점에서 신속하게 찾을 수 있습니다.  

- 더 나아가, 제조업 강국인 대한민국은 **인구 절벽 문제**로 인해 노동력 부족 현상이 심화되고 있으며,  
  이에 따라 로봇의 일자리 대체는 **불가피한 흐름**이 되고 있습니다.  

- 따라서, **AI 기반 자동화의 품질 개선과 문제 포착을 손쉽게 지원하는 에이전트 툴**이 필요하다고 판단했습니다.  

---

## 🧪 시도한 접근 & 결과

### ✅ 성공한 시도
- **SigLIP 기반 이미지 임베딩 검색**
  - 영상 프레임을 SigLIP(1152-dim)으로 임베딩하여 텍스트-이미지 검색을 구현
  - 대량의 로봇 영상 데이터를 효율적으로 탐색 가능
  - **멀티모달 검색 파이프라인의 핵심 기능으로 채택**
  - → 결론적으로, 비주얼 데이터(영상)에서는 임베딩 검색이 가장 실용적이고 성능도 안정적임을 확인

---

### ⚠️ 한계/실패한 시도

- **머신러닝 기반 센서 시퀀스 분류**
  - 적용 알고리즘: Logistic Regression, Random Forest, **XGBoost**
  - 센서 시퀀스를 구간 단위(feature 집계)로 변환 후 분류 시도
  - 결과: 일정 수준의 성능은 보였으나,
    - **라벨 경계가 애매**하여 안정적 정확도 확보 실패
    - XGBoost는 비교적 나은 성능을 보였지만 여전히 분류 한계 존재
  - 인사이트: 센서 데이터만으로는 “동작 전환 경계”를 잘 포착하기 어려움  
  - 추가 성과: **Feature Importance 분석**을 통해 수많은 센서 컬럼 중 유효한 특성을 선별할 수 있었음 → 이후 검색/탐색 설계에 참고됨  

- **딥러닝 기반 센서 시퀀스 분류**
  - `SensorLSTMAttnClassifier` (LSTM + Attention Pooling)
  - 시도: 시퀀스 전체를 입력받아 Attention 기반 요약 후 `desc_major` 분류
  - 결과: 구조적으로는 정상 작동했으나
    - **라벨 경계 불명확성**으로 학습 안정성이 낮음
    - **클래스 불균형**으로 특정 라벨에 치우친 결과 발생
    - 장기 시퀀스 일반화 부족
  - 인사이트: 센서 데이터는 분류보다는 **검색/탐색 활용**이 더 적합하다는 결론

#### 📉 센서 기반 분류 결과
LSTM + Attention 모델을 적용했으나, 라벨 경계 문제와 클래스 불균형으로 인해 성능이 제한적이었습니다.

![xgboost + 로봇 행동 분류 성능](https://github.com/user-attachments/assets/33dde14a-575f-4b6a-9413-91c44d9c81b1)



- 일부 클래스(F1 ≈ 0.7)는 양호했지만, `"정렬"`과 같은 라벨은 F1 ≈ 0.2 수준  
- 전체 accuracy ≈ 0.50 → **실제 적용하기에는 부족**  
- ➡️ 분류보다는 검색 접근이 더 적합함을 확인  

---

## 🚀 해결책
이 프로젝트는 **멀티모달 검색 에이전트**를 제공합니다.  

- **입력**: 텍스트, 키워드, 혹은 예시 영상/센서 구간  
- **검색 대상**: 로봇 센서 데이터 (Cartesian Position, Velocity, Joint State 등) + 동기화된 영상  
- **출력**: 쿼리와 가장 유사한 Top5 영상 + 해당 영상의 센서 데이터 통계 정보  

---

## ✨ 기대 효과
- 🔍 **효율적인 탐색**  
  방대한 로봇 데이터셋에서 원하는 동작을 빠르게 찾을 수 있음  
- 🎥 **멀티모달 검색**  
  텍스트 + 영상 + 센서 신호를 동시에 활용하여 검색 정밀도 향상  
- 🛠 **엔지니어링 생산성 향상**  
  반복적이고 시간이 많이 드는 데이터 탐색 과정을 자동화  
- 📈 **연구/실험 가속화**  
  모델 학습이나 분석에 필요한 데이터를 더 쉽게 수집 가능  

---

## 🛠 기술 스택
- **Dataset**: DROID Dataset 1.0.1 (로봇 시퀀스/비디오/센서 로그)  
- **Backend**: Python, FastAPI  
- **Frontend**: React + TypeScript  
- **Image Embedding**: SigLIP 1152-dim  
- **Search Engine**: Elasticsearch 기반 멀티모달 인덱싱 (텍스트+센서+영상 프레임)  
- **Agent Tool**: 데이터 검색 및 탐색 자동화  

---

## 📌 결론
이 프로젝트는 **"로봇 동작 데이터의 애매한 경계 문제"**에서 출발했지만  
결국에는 **대량의 로봇 데이터와 영상에서 원하는 행동을 빠르게 찾는 검색 Agent**로 발전했습니다.  

👉 이를 통해 확인한 점:  
- 단순 분류 접근은 라벨/데이터 한계로 인해 **상용 수준의 성능을 내기 어려움**  
- 그러나 검색 기반 접근은 **엔지니어가 실제로 필요로 하는 문제 해결에 직결**  
- 따라서, 이 프로젝트는 **로봇 데이터 활용의 실질적 가치를 극대화**하는 방향으로 자리 잡았습니다.
